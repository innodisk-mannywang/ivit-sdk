# iVIT SDK é–‹ç™¼è·¯ç·šåœ–

> **æ›´æ–°æ—¥æœŸ**ï¼š2026-01-26

## é–‹ç™¼é€²åº¦ç¸½è¦½

| Phase | ç‹€æ…‹ | å®Œæˆåº¦ |
|-------|------|--------|
| Phase 1: API ç°¡åŒ– | âœ… å®Œæˆ | 100% |
| Phase 2: é€²éšæ§åˆ¶ | âœ… å®Œæˆ | 100% |
| Phase 3: é–‹ç™¼é«”é©— | âœ… å®Œæˆ | 100% |
| Phase 4: ç”Ÿæ…‹å»ºè¨­ | âœ… å®Œæˆ | 100% |

---

## å®šä½ç­–ç•¥

### èˆ‡ç«¶å“çš„å·®ç•°åŒ–

| SDK | å®šä½ | iVIT çš„æ©Ÿæœƒ |
|-----|------|------------|
| **Ultralytics** | YOLO è¨“ç·´ + æ¨è«– | iVIT å°ˆæ³¨éƒ¨ç½²ï¼Œä¸åšè¨“ç·´ |
| **OpenVINO** | Intel ç¡¬é«”å°ˆç”¨ | iVIT è·¨å¹³å°çµ±ä¸€ |
| **TensorRT** | NVIDIA ç¡¬é«”å°ˆç”¨ | iVIT è·¨å¹³å°çµ±ä¸€ |

**æ ¸å¿ƒåƒ¹å€¼ä¸»å¼µ**ï¼šã€Œç”¨ä»»ä½•æ¡†æ¶è¨“ç·´ï¼Œç”¨ iVIT éƒ¨ç½²åˆ°ä»»ä½•ç¡¬é«”ã€

---

## åŠŸèƒ½å„ªåŒ–è·¯ç·šåœ–

### Phase 1ï¼šAPI ç°¡åŒ–ï¼ˆå­¸ç¿’ Ultralyticsï¼‰âœ…

#### 1.1 æ¥µç°¡ APIï¼ˆL1ï¼‰

**ç›®æ¨™**ï¼šä¸€è¡Œè¼‰å…¥ã€ä¸€è¡Œæ¨è«–

```python
# ç¾åœ¨
from ivit.vision import Detector
detector = Detector("yolov8n.onnx", device="cuda:0")
results = detector.predict(image)

# å„ªåŒ–å¾Œ
import ivit
model = ivit.load("yolov8n.onnx")
results = model("image.jpg")  # è‡ªå‹•åµæ¸¬ä»»å‹™é¡å‹
results.show()
```

**å·²å®Œæˆ** âœ…ï¼š
- [x] `ivit.load()` çµ±ä¸€è¼‰å…¥å‡½æ•¸
- [x] è‡ªå‹•ä»»å‹™é¡å‹åµæ¸¬ï¼ˆåˆ†é¡/åµæ¸¬/åˆ†å‰²ï¼‰
- [x] æ”¯æ´ç›´æ¥å‚³å…¥æª”æ¡ˆè·¯å¾‘
- [x] `results.show()` å¿«é€Ÿè¦–è¦ºåŒ–
- [x] `results.save()` å¿«é€Ÿå„²å­˜
- [x] `model()` ç›´æ¥å‘¼å«æ¨è«–ï¼ˆUltralytics é¢¨æ ¼ï¼‰

#### 1.2 åƒæ•¸é…ç½®ï¼ˆL2ï¼‰

**ç›®æ¨™**ï¼šè±å¯Œçš„æ¨è«–åƒæ•¸

```python
results = model.predict(
    source="image.jpg",
    conf=0.25,           # ä¿¡å¿ƒåº¦é–¾å€¼
    iou=0.45,            # NMS IoU é–¾å€¼
    max_det=300,         # æœ€å¤§åµæ¸¬æ•¸
    classes=[0, 1, 2],   # åªåµæ¸¬ç‰¹å®šé¡åˆ¥
    imgsz=640,           # è¼¸å…¥å°ºå¯¸
    half=True,           # FP16 æ¨è«–
    augment=True,        # æ¸¬è©¦æ™‚å¢å¼·
    save=True,           # å„²å­˜çµæœ
    stream=True,         # ä¸²æµæ¨¡å¼
)
```

**å·²å®Œæˆ** âœ…ï¼š
- [x] çµ±ä¸€çš„æ¨è«–åƒæ•¸ä»‹é¢ (`conf`, `iou`, `classes`, `max_det`)
- [x] ä¸²æµæ¨¡å¼ `model.stream()` generator æ¨¡å¼
- [x] æ‰¹æ¬¡æ¨è«–æ”¯æ´ `model.predict_batch()`
- [x] æ¸¬è©¦æ™‚å¢å¼· `model.predict_tta()` (TTA)

**TTA ä½¿ç”¨ç¯„ä¾‹**ï¼š
```python
# åŸºæœ¬ TTAï¼ˆæ°´å¹³ç¿»è½‰ï¼‰
results = model.predict_tta("image.jpg")

# å¤šå°ºåº¦ TTA
results = model.predict_tta("image.jpg", scales=[0.8, 1.0, 1.2])

# è‡ªè¨‚å¢å¼·çµ„åˆ
results = model.predict_tta(
    "image.jpg",
    augments=["original", "hflip", "rotate90"],
    scales=[0.8, 1.0, 1.2]
)
```

---

### Phase 2ï¼šé€²éšæ§åˆ¶ âœ…

#### 2.1 Callback ç³»çµ±ï¼ˆL3ï¼‰âœ…

**å·²å®Œæˆ**ï¼šæä¾›æ“´å±•é»ï¼Œä¸æ”¹æ ¸å¿ƒç¨‹å¼ç¢¼

```python
# ä½¿ç”¨è£é£¾å™¨è¨»å†Š
@model.on("infer_end")
def log_metrics(ctx):
    print(f"Latency: {ctx.latency_ms}ms")
    print(f"Preprocess: {ctx.preprocess_ms}ms")
    print(f"Inference: {ctx.inference_ms}ms")

# ä½¿ç”¨ lambda
model.on("pre_process", lambda ctx: print(f"Image shape: {ctx.image_shape}"))

# å…§å»º callbacks
from ivit.core.callbacks import FPSCounter, LatencyLogger, DetectionFilter
fps = FPSCounter(window_size=30)
model.on("infer_end", fps)
print(fps.fps)  # Get current FPS
```

**å·²å¯¦ä½œ**ï¼š
- [x] `model.on()` Callback è¨»å†Šï¼ˆè£é£¾å™¨ + ç›´æ¥å‘¼å«ï¼‰
- [x] `model.remove_callback()` ç§»é™¤ callback
- [x] `CallbackContext` åŒ…å«å®Œæ•´æ¨è«–è³‡è¨Š
- [x] æ”¯æ´çš„äº‹ä»¶ï¼š
  - `pre_process` - å‰è™•ç†å‰
  - `post_process` - å¾Œè™•ç†å¾Œ
  - `infer_start` - æ¨è«–é–‹å§‹
  - `infer_end` - æ¨è«–çµæŸï¼ˆå«è©³ç´°æ™‚é–“ï¼‰
  - `batch_start/batch_end` - æ‰¹æ¬¡è™•ç†
  - `stream_start/frame/end` - ä¸²æµè™•ç†
- [x] å…§å»º callbacksï¼š`FPSCounter`, `LatencyLogger`, `DetectionFilter`

#### 2.2 ç¡¬é«”ç‰¹å®šé…ç½®ï¼ˆL4ï¼‰âœ…

**å·²å®Œæˆ**ï¼šæš´éœ²åº•å±¤å„ªåŒ–é¸é …

```python
# OpenVINO ç‰¹å®šè¨­å®š
model.configure_openvino(
    performance_mode="LATENCY",
    num_streams=4,
    inference_precision="FP16",
    enable_cpu_pinning=True,
)

# TensorRT ç‰¹å®šè¨­å®š
model.configure_tensorrt(
    workspace_size=1 << 30,
    dla_core=0,
    enable_sparsity=True,
    builder_optimization_level=3,
)

# ONNX Runtime è¨­å®š
model.configure_onnxruntime(
    num_threads=8,
    enable_cuda_graph=True,
)

# æ”¯æ´ method chaining
model.configure_onnxruntime(num_threads=4).warmup(3)
```

**å·²å¯¦ä½œ**ï¼š
- [x] `configure_openvino()` æ–¹æ³•
- [x] `configure_tensorrt()` æ–¹æ³•
- [x] `configure_onnxruntime()` æ–¹æ³•
- [x] `configure_snpe()` æ–¹æ³•
- [x] é…ç½®é¡åˆ¥ï¼š`OpenVINOConfig`, `TensorRTConfig`, `ONNXRuntimeConfig`, `SNPEConfig`

#### 2.3 åº•å±¤å­˜å–ï¼ˆL5ï¼‰âœ…

**å·²å®Œæˆ**ï¼šå°ˆå®¶å¯ç›´æ¥æ“ä½œåº•å±¤

```python
# å­˜å– runtime
runtime = model.runtime
print(runtime.name)  # "onnxruntime"

# å­˜å–åº•å±¤ handle
handle = model.runtime_handle
session = handle.session  # ONNX InferenceSession

# åŸå§‹æ¨è«–ï¼ˆç„¡å‰å¾Œè™•ç†ï¼‰
input_name = model.input_info[0]["name"]
outputs = model.infer_raw({input_name: my_tensor})
```

**å·²å¯¦ä½œ**ï¼š
- [x] `model.runtime` å±¬æ€§
- [x] `model.runtime_handle` å±¬æ€§
- [x] `model.infer_raw()` åŸå§‹æ¨è«–æ–¹æ³•

---

### Phase 3ï¼šé–‹ç™¼é«”é©— âœ…

#### 3.1 è£ç½®ç®¡ç†ï¼ˆå·²å®Œæˆ âœ…ï¼‰

```python
# åˆ—å‡ºè£ç½®
ivit.devices()
ivit.devices.summary()

# å–å¾—ç‰¹å®šè£ç½®
ivit.devices.cuda()
ivit.devices.cpu()
ivit.devices.npu()
ivit.devices.best()
ivit.devices.best("efficiency")
```

#### 3.2 CLI å·¥å…·å¢å¼· âœ…

**å·²å®Œæˆ**ï¼š
- [x] `ivit info` - é¡¯ç¤ºç³»çµ±è³‡è¨Š
- [x] `ivit devices` - åˆ—å‡ºè£ç½®
- [x] `ivit benchmark` - æ•ˆèƒ½æ¸¬è©¦
- [x] `ivit predict` - åŸ·è¡Œæ¨è«–
- [x] `ivit convert` - æ¨¡å‹è½‰æ›ï¼ˆOpenVINOã€TensorRTï¼‰
- [x] `ivit serve` - å•Ÿå‹•æ¨è«–æœå‹™
- [x] `ivit export` - åŒ¯å‡ºæ¨¡å‹
- [x] `ivit zoo` - Model Zoo æ“ä½œ

#### 3.3 éŒ¯èª¤è¨Šæ¯å„ªåŒ– âœ…

**å·²å®Œæˆ**ï¼šå‹å–„çš„éŒ¯èª¤æç¤º

```python
# å„ªåŒ–å¾Œçš„éŒ¯èª¤è¨Šæ¯ç¯„ä¾‹
ivit.ModelLoadError: ç„¡æ³•è¼‰å…¥æ¨¡å‹ 'model.onnx'

å¯èƒ½åŸå› ï¼š
  1. æª”æ¡ˆä¸å­˜åœ¨ï¼šè«‹ç¢ºèªè·¯å¾‘ '/path/to/model.onnx' æ˜¯å¦æ­£ç¢º
  2. æ ¼å¼ä¸æ”¯æ´ï¼šç›®å‰æ”¯æ´ .onnx, .xml, .engine
  3. å¾Œç«¯ä¸å¯ç”¨ï¼šTensorRT éœ€è¦ NVIDIA GPU

è§£æ±ºå»ºè­°ï¼š
  - åŸ·è¡Œ `ivit.devices()` ç¢ºèªå¯ç”¨ç¡¬é«”
  - åŸ·è¡Œ `ivit convert model.onnx model.engine` è½‰æ›æ ¼å¼
```

**å·²å¯¦ä½œ**ï¼š
- [x] `IVITError` åŸºç¤éŒ¯èª¤é¡åˆ¥
- [x] `ModelLoadError` æ¨¡å‹è¼‰å…¥éŒ¯èª¤
- [x] `DeviceNotFoundError` è£ç½®ä¸å¯ç”¨
- [x] `BackendNotAvailableError` å¾Œç«¯æœªå®‰è£
- [x] `InferenceError` æ¨è«–éŒ¯èª¤
- [x] `InvalidInputError` è¼¸å…¥æ ¼å¼éŒ¯èª¤
- [x] `ConfigurationError` é…ç½®éŒ¯èª¤
- [x] `ModelConversionError` æ¨¡å‹è½‰æ›éŒ¯èª¤
- [x] `ResourceExhaustedError` è³‡æºä¸è¶³
- [x] `wrap_error()` åŒ…è£é€šç”¨éŒ¯èª¤

---

### Phase 4ï¼šç”Ÿæ…‹å»ºè¨­ âœ…

#### 4.1 Model Zoo âœ…

**å·²å®Œæˆ**ï¼š

```python
# å¾ Model Zoo è¼‰å…¥
model = ivit.zoo.load("yolov8n")  # è‡ªå‹•ä¸‹è¼‰ + è½‰æ›
model = ivit.zoo.load("yolov8n", device="cuda:0")

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
ivit.zoo.list_models()              # 14 å€‹é è¨­æ¨¡å‹
ivit.zoo.list_models(task="detect") # æŒ‰ä»»å‹™éæ¿¾
ivit.zoo.search("yolo")             # æœå°‹æ¨¡å‹
ivit.zoo.search("edge")             # æ‰¾é‚Šç·£è£ç½®æ¨¡å‹

# å–å¾—æ¨¡å‹è³‡è¨Š
info = ivit.zoo.get_model_info("yolov8n")
print(f"Task: {info.task}, Input: {info.input_size}")
```

**å·²æ”¯æ´æ¨¡å‹**ï¼š
| é¡å‹ | æ¨¡å‹ |
|------|------|
| Detection | yolov8n/s/m/l/x |
| Classification | yolov8n-cls, yolov8s-cls, resnet50, mobilenetv3, efficientnet-b0 |
| Segmentation | yolov8n-seg, yolov8s-seg |
| Pose | yolov8n-pose, yolov8s-pose |

#### 4.2 æ¨¡å‹è¨“ç·´ âœ…

**å·²å®Œæˆ**ï¼šè‡ªè¡Œå¯¦ä½œé·ç§»å¼å­¸ç¿’è¨“ç·´åŠŸèƒ½

```python
import ivit
from ivit.train import Trainer, ImageFolderDataset, EarlyStopping, ModelCheckpoint

# å»ºç«‹è¨“ç·´è³‡æ–™é›†
dataset = ImageFolderDataset("./my_dataset", train_split=0.8, split="train")
val_dataset = ImageFolderDataset("./my_dataset", train_split=0.8, split="val")

# è¨“ç·´åˆ†é¡æ¨¡å‹
trainer = Trainer(
    model="resnet50",           # æ”¯æ´ resnet18/34/50/101, efficientnet_b0-b2, mobilenet_v2/v3
    dataset=dataset,
    val_dataset=val_dataset,
    epochs=20,
    learning_rate=0.001,
    batch_size=32,
    device="cuda:0",
    freeze_backbone=True,       # é·ç§»å¼å­¸ç¿’ï¼šå‡çµéª¨å¹¹ç¶²è·¯
)

# è¨“ç·´ä¸¦ä½¿ç”¨å›èª¿
trainer.fit(callbacks=[
    EarlyStopping(patience=5),
    ModelCheckpoint("best_model.pt"),
])

# è©•ä¼°æ¨¡å‹
metrics = trainer.evaluate()
print(f"Accuracy: {metrics['accuracy']:.2%}")

# åŒ¯å‡ºæ¨¡å‹
trainer.export("my_model.onnx", format="onnx", quantize="fp16")
```

**å·²å¯¦ä½œ**ï¼š
- [x] `ivit.train.ImageFolderDataset` - ImageFolder æ ¼å¼è³‡æ–™é›†
- [x] `ivit.train.COCODataset` - COCO æ ¼å¼è³‡æ–™é›†
- [x] `ivit.train.YOLODataset` - YOLO æ ¼å¼è³‡æ–™é›†
- [x] `ivit.train.Trainer` - è¨“ç·´å™¨ï¼ˆé·ç§»å¼å­¸ç¿’ï¼‰
- [x] `ivit.train.Augmentation` - è³‡æ–™å¢å¼·ï¼ˆResize, Flip, Rotation, ColorJitter, Normalizeï¼‰
- [x] è¨“ç·´å›èª¿ï¼šEarlyStopping, ModelCheckpoint, ProgressLogger, LRScheduler, TensorBoardLogger
- [x] æ¨¡å‹åŒ¯å‡ºï¼šONNX, TorchScript, OpenVINO IR, TensorRT Engine
- [x] é‡åŒ–æ”¯æ´ï¼šFP16, INT8ï¼ˆéœ€è¦æ ¡æ­£è³‡æ–™ï¼‰
- [x] æ”¯æ´ 14+ é è¨“ç·´æ¨¡å‹ï¼ˆResNet, EfficientNet, MobileNet, VGG, DenseNetï¼‰

#### 4.3 æ–‡ä»¶å’Œç¯„ä¾‹ âœ…

**å·²å®Œæˆ**ï¼š
- [x] API åƒè€ƒæ–‡ä»¶ (`docs/api/API-SPEC-001-ivit-sdk.md` v1.1)
- [x] å¿«é€Ÿå…¥é–€æŒ‡å— (`docs/GETTING_STARTED.md`)
- [x] å„å¹³å°éƒ¨ç½²ç¯„ä¾‹ï¼š
  - [x] Intel OpenVINO (`docs/deployment/INTEL_OPENVINO.md`)
  - [x] NVIDIA TensorRT (`docs/deployment/NVIDIA_TENSORRT.md`)
  - [x] Qualcomm SNPE (`docs/deployment/QUALCOMM_SNPE.md`)
- [x] æ•ˆèƒ½èª¿å„ªæŒ‡å— (`docs/PERFORMANCE_TUNING.md`)
- [x] å¸¸è¦‹å•é¡Œ FAQ (`docs/FAQ.md`)

**å·²å®Œæˆ** âœ…ï¼š
- [x] è¨“ç·´åŠŸèƒ½æ•™å­¸ï¼ˆivit.train æ¨¡çµ„ï¼‰

---

## TODO: å¾…ä¿®å¾©å•é¡Œ

### ğŸ”´ é«˜å„ªå…ˆç´šï¼šæ¨è«–å¼•æ“å„ªå…ˆé †åºä¿®æ­£

**å•é¡Œæè¿°**ï¼šç›®å‰ SDK åœ¨ NVIDIA GPU ä¸Šé è¨­ä½¿ç”¨ ONNX Runtime + CUDAExecutionProviderï¼Œè€Œéæ•ˆèƒ½æ›´ä½³çš„ TensorRTã€‚

**æ ¹å› åˆ†æ**ï¼š
```
get_backend_for_device("cuda:0")
    â†“
_tensorrt_available() æª¢æŸ¥å¤±æ•—ï¼ˆéœ€è¦ tensorrt + pycudaï¼‰
    â†“
å›é€€åˆ° _onnxruntime_cuda_available()
    â†“
ä½¿ç”¨ ONNX Runtimeï¼ˆæ•ˆèƒ½è¼ƒå·®ï¼‰
```

**æœŸæœ›è¡Œç‚º**ï¼š
| ç¡¬é«”å¹³å° | ç¬¬ä¸€å„ªå…ˆ | ç¬¬äºŒå„ªå…ˆï¼ˆå›é€€ï¼‰ |
|----------|----------|------------------|
| NVIDIA GPU | TensorRT | ONNX Runtime + CUDA |
| Intel CPU/iGPU/NPU | OpenVINO | ONNX Runtime |
| Qualcomm NPU | SNPE/QNN | ONNX Runtime |
| é€šç”¨ CPU | OpenVINO | ONNX Runtime |

**ä¿®æ”¹ä½ç½®**ï¼š
- `python/ivit/core/device.py`: `get_backend_for_device()` å‡½æ•¸
- `python/ivit/devices.py`: è£ç½®æ¢ç´¢æ™‚æ¨™è¨˜å¯¦éš›å¯ç”¨çš„å¾Œç«¯
- `python/ivit/runtime/__init__.py`: å¢åŠ æ›´ç´°ç·»çš„å¾Œç«¯å¯ç”¨æ€§æª¢æŸ¥

**é ä¼°æ•ˆèƒ½æå‡**ï¼š
- TensorRT vs ONNX Runtime + CUDA: 2-3x é€Ÿåº¦æå‡
- OpenVINO vs ONNX Runtime (CPU): 1.5-2x é€Ÿåº¦æå‡

---

## æ•ˆèƒ½å„ªåŒ–

### SDK åŒ…è£å±¤é–‹éŠ·ç›®æ¨™

| æŒ‡æ¨™ | ç›®å‰ | ç›®æ¨™ |
|------|------|------|
| Python å‘¼å«é–‹éŠ· | ~0.3ms | <0.1ms |
| C++ å‘¼å«é–‹éŠ· | ~0.05ms | <0.01ms |
| è¨˜æ†¶é«”é¡å¤–é–‹éŠ· | ~10MB | <5MB |

### å„ªåŒ–æ–¹å‘

- [ ] æ¸›å°‘ Python/C++ é‚Šç•Œçš„è³‡æ–™è¤‡è£½
- [ ] ä½¿ç”¨ buffer protocol é›¶è¤‡è£½å‚³è¼¸
- [ ] é åˆ†é…è¼¸å‡º buffer
- [ ] æ‰¹æ¬¡æ¨è«–å„ªåŒ–

---

## æ™‚ç¨‹è¦åŠƒ

| Phase | å…§å®¹ | é ä¼°æ™‚é–“ |
|-------|------|---------|
| Phase 1 | API ç°¡åŒ– | 4 é€± |
| Phase 2 | é€²éšæ§åˆ¶ | 4 é€± |
| Phase 3 | é–‹ç™¼é«”é©— | 4 é€± |
| Phase 4 | ç”Ÿæ…‹å»ºè¨­ | æŒçºŒ |

---

## åƒè€ƒè³‡æ–™

- [Ultralytics API è¨­è¨ˆ](https://docs.ultralytics.com/)
- [OpenVINO Python API](https://docs.openvino.ai/latest/api/ie_python_api.html)
- [TensorRT Python API](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/)
